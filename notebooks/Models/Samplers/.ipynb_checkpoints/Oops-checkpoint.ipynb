{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3746c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass,asdict\n",
    "\n",
    "from graph_bridges.data.transforms import SpinsToBinaryTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0aee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_bridges.models.spin_glass.ising_parameters import ParametrizedSpinGlassHamiltonianConfig\n",
    "from graph_bridges.models.spin_glass.ising_parameters import ParametrizedSpinGlassHamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc466507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_bridges.configs.spin_glass.spin_glass_config_sb import SBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89b54920",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_spins = 3\n",
    "number_of_paths = 20\n",
    "batch_size = 32\n",
    "number_of_mcmc_steps = 500\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "config = ParametrizedSpinGlassHamiltonianConfig()\n",
    "number_of_couplings = ParametrizedSpinGlassHamiltonian.coupling_size(number_of_spins)\n",
    "\n",
    "#fields = torch.Tensor(size=(number_of_spins,)).normal_(0.,1./number_of_spins)\n",
    "#couplings = torch.Tensor(size=(number_of_couplings,)).normal_(0.,1/number_of_spins)\n",
    "fields, couplings = ParametrizedSpinGlassHamiltonian.sample_random_model(number_of_spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e3f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.number_of_spins = number_of_spins\n",
    "config.couplings_deterministic =  None\n",
    "config.obtain_partition_function = False\n",
    "config.number_of_mcmc_steps = number_of_mcmc_steps\n",
    "config.number_of_paths = number_of_paths\n",
    "config.couplings_sigma = 5.\n",
    "config.fields = fields\n",
    "config.couplings = couplings\n",
    "\n",
    "ising_model_real = ParametrizedSpinGlassHamiltonian(config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34067ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SBConfig()\n",
    "config.align_configurations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "922b49d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 3.6454, -0.3805, -3.7722], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ising_model_real.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e7e2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BilinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BilinearLayer, self).__init__()\n",
    "        # Define the matrix A as a parameter of the module\n",
    "        self.A = nn.Parameter(torch.rand(input_dim, input_dim))  # You can initialize it as you like\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the bilinear form x^T A x\n",
    "        bilinear_form = torch.matmul(x, torch.matmul(self.A, x))\n",
    "        return bilinear_form\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 3  # Change this according to your input dimension\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Create the bilinear layer\n",
    "bilinear_layer = BilinearLayer(input_dim)\n",
    "\n",
    "# Forward pass to calculate the bilinear form\n",
    "output = bilinear_layer(x)\n",
    "\n",
    "# Compute gradients\n",
    "output.backward()\n",
    "\n",
    "# Access the gradients of A\n",
    "gradient_A = bilinear_layer.A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1ced261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coupling from Spin File Different from Config\n",
      "Coupling from Spin File Different from Config\n",
      "Coupling from Spin File Different from Config\n",
      "Coupling from Spin File Different from Config\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from graph_bridges.models.generative_models.sb import SB\n",
    "from graph_bridges.configs.config_sb import SBTrainerConfig\n",
    "from graph_bridges.data.graph_dataloaders_config import EgoConfig\n",
    "from graph_bridges.data.spin_glass_dataloaders_config import ParametrizedSpinGlassHamiltonianConfig\n",
    "from graph_bridges.configs.graphs.graph_config_sb import SBConfig\n",
    "from graph_bridges.configs.config_sb import ParametrizedSamplerConfig, SteinSpinEstimatorConfig\n",
    "\n",
    "from graph_bridges.models.temporal_networks.mlp.temporal_mlp import TemporalMLPConfig\n",
    "from graph_bridges.models.backward_rates.ctdd_backward_rate_config import BackwardRateTemporalHollowTransformerConfig\n",
    "from graph_bridges.models.temporal_networks.transformers.temporal_hollow_transformers import TemporalHollowTransformerConfig\n",
    "from graph_bridges.models.trainers.sb_training import SBTrainer\n",
    "from graph_bridges.models.spin_glass.spin_utils import copy_and_flip_spins\n",
    "\n",
    "from graph_bridges.data.transforms import SpinsToBinaryTensor\n",
    "spins_to_binary = SpinsToBinaryTensor()\n",
    "\n",
    "sb_config = SBConfig(delete=True,\n",
    "                     experiment_name=\"spin_glass\",\n",
    "                     experiment_type=\"sb\",\n",
    "                     experiment_indentifier=None)\n",
    "\n",
    "#self.sb_config.data = EgoConfig(as_image=False, batch_size=2, flatten_adjacency=True,full_adjacency=True)\n",
    "num_epochs = 200\n",
    "batch_size = 2\n",
    "\n",
    "sb_config.data = ParametrizedSpinGlassHamiltonianConfig(data=\"bernoulli_small\",\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        number_of_spins=3)\n",
    "sb_config.target = ParametrizedSpinGlassHamiltonianConfig(data=\"bernoulli_small\",\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          number_of_spins=3)\n",
    "\n",
    "sb_config.temp_network = TemporalMLPConfig(time_embed_dim=12,hidden_dim=250)\n",
    "sb_config.stein = SteinSpinEstimatorConfig(stein_sample_size=200,stein_epsilon=1e-2)\n",
    "sb_config.sampler = ParametrizedSamplerConfig(num_steps=20)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "sb = SB()\n",
    "sb.create_new_from_config(sb_config,device)\n",
    "\n",
    "batchdata = next(sb.data_dataloader.train().__iter__())\n",
    "X_spins = batchdata[0]\n",
    "X_copy_spin, X_flipped_spin = copy_and_flip_spins(X_spins)\n",
    "X_spins = X_spins.to(device)\n",
    "X_copy_spin = X_copy_spin.to(device)\n",
    "X_flipped_spin = X_flipped_spin.to(device)\n",
    "\n",
    "fake_time = torch.rand((X_copy_spin.shape[0],)).to(device)\n",
    "current_model = sb.training_model\n",
    "phi_0 = current_model(X_copy_spin,fake_time)\n",
    "phi_1 = current_model(X_flipped_spin,fake_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "84fa6172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.],\n",
       "        [ 1., -1.,  1.],\n",
       "        [ 1., -1.,  1.],\n",
       "        [ 1., -1., -1.],\n",
       "        [ 1., -1., -1.],\n",
       "        [ 1., -1., -1.]], device='cuda:0')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_copy_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6f7e382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def compute_jacobian(model, input_data):\n",
    "    input_data.requires_grad_(True)\n",
    "    output = model(input_data)\n",
    "    batch_size, num_outputs = output.size(0), output.size(1)\n",
    "    jacobian = torch.zeros(batch_size, num_outputs, input_data.size(1))\n",
    "    \n",
    "    for i in range(num_outputs):\n",
    "        output_i = output[:, i]\n",
    "        gradient = torch.zeros_like(output_i)\n",
    "        gradient.fill_(1.0)\n",
    "        output_i.backward(gradient, retain_graph=True)\n",
    "        jacobian[:, i, :] = input_data.grad.clone()\n",
    "        input_data.grad.zero_()\n",
    "    \n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f617d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLP\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "\n",
    "hidden_dim = 10\n",
    "batch_size = 23\n",
    "input_data = torch.randn(batch_size, input_dim)\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "X_spins = X_spins.to(torch.device(\"cpu\"))\n",
    "mlp = MLP(input_dim, hidden_dim, output_dim)\n",
    "spins = spins_to_binary(X_spins)\n",
    "copy_spin = spins_to_binary(X_copy_spin).to(cpu_device)\n",
    "flipped_spin = spins_to_binary(X_flipped_spin).to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b0045719",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = compute_jacobian(mlp, spins)\n",
    "J = J.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f2be9c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1692,  0.0123,  0.0648],\n",
       "        [-0.1697,  0.0094,  0.0722]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c51e95a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1692,  0.0123, -0.0648],\n",
       "        [ 0.1697,  0.0094,  0.0722]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(2.*spins - 1.)*J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6292d4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0799],\n",
       "        [ 0.0142],\n",
       "        [-0.0680],\n",
       "        [ 0.1733],\n",
       "        [ 0.0166],\n",
       "        [ 0.0680]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(flipped_spin) - mlp(copy_spin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5071f1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.],\n",
       "        [1., 0., 1.],\n",
       "        [1., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8180212e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 0., 1.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gibbs_with_gradients(unnormalized_log_prob, current_sample, num_samples=1):\n",
    "    samples = []\n",
    "    x = current_sample.clone()  # Make a copy of the current sample\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Compute d_tilde(x)\n",
    "        d_tilde = torch.sigmoid(x)  # For binary data, Eq. 3\n",
    "        \n",
    "        # Compute q(i|x)\n",
    "        q_i_given_x = F.softmax(d_tilde, dim=0)\n",
    "        \n",
    "        # Sample i ~ q(i|x)\n",
    "        i = torch.multinomial(q_i_given_x, 1).item()\n",
    "        \n",
    "        # Flip dimension i\n",
    "        x_flipped = x.clone()\n",
    "        x_flipped[i] = 1 - x[i]\n",
    "        \n",
    "        # Compute d_tilde(x_flipped)\n",
    "        d_tilde_flipped = torch.sigmoid(x_flipped)  # For binary data, Eq. 3\n",
    "        \n",
    "        # Compute q(i|x_flipped)\n",
    "        q_i_given_x_flipped = F.softmax(d_tilde_flipped, dim=0)\n",
    "        \n",
    "        # Compute the acceptance probability\n",
    "        acceptance_prob = min(1, torch.exp(unnormalized_log_prob(x_flipped) - unnormalized_log_prob(x)) * \n",
    "                                   q_i_given_x_flipped[i] / q_i_given_x[i])\n",
    "        \n",
    "        # Accept the new sample with probability acceptance_prob\n",
    "        if torch.rand(1).item() < acceptance_prob:\n",
    "            x = x_flipped\n",
    "        \n",
    "        samples.append(x.clone())\n",
    "    \n",
    "    return torch.stack(samples)\n",
    "\n",
    "# Example usage:\n",
    "# Define your unnormalized log-prob function unnormalized_log_prob(x)\n",
    "# Initialize a current sample x_current\n",
    "# Call the gibbs_with_gradients function to perform Gibbs sampling\n",
    "# sampled_samples = gibbs_with_gradients(unnormalized_log_prob, x_current, num_samples=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
