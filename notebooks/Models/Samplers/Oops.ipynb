{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3746c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass,asdict\n",
    "\n",
    "from graph_bridges.data.transforms import SpinsToBinaryTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0aee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_bridges.models.spin_glass.ising_parameters import ParametrizedSpinGlassHamiltonianConfig\n",
    "from graph_bridges.models.spin_glass.ising_parameters import ParametrizedSpinGlassHamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc466507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_bridges.configs.spin_glass.spin_glass_config_sb import SBConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "577ff615",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_spins = 3\n",
    "number_of_paths = 20\n",
    "batch_size = 32\n",
    "number_of_mcmc_steps = 500\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "config = ParametrizedSpinGlassHamiltonianConfig()\n",
    "number_of_couplings = ParametrizedSpinGlassHamiltonian.coupling_size(number_of_spins)\n",
    "\n",
    "#fields = torch.Tensor(size=(number_of_spins,)).normal_(0.,1./number_of_spins)\n",
    "#couplings = torch.Tensor(size=(number_of_couplings,)).normal_(0.,1/number_of_spins)\n",
    "fields, couplings = ParametrizedSpinGlassHamiltonian.sample_random_model(number_of_spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92cda0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.number_of_spins = number_of_spins\n",
    "config.couplings_deterministic =  None\n",
    "config.obtain_partition_function = False\n",
    "config.number_of_mcmc_steps = number_of_mcmc_steps\n",
    "config.number_of_paths = number_of_paths\n",
    "config.couplings_sigma = 5.\n",
    "config.fields = fields\n",
    "config.couplings = couplings\n",
    "\n",
    "ising_model_real = ParametrizedSpinGlassHamiltonian(config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92c36340",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SBConfig()\n",
    "config.align_configurations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f42d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 3.6454, -0.3805, -3.7722], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ising_model_real.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "988a168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BilinearLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BilinearLayer, self).__init__()\n",
    "        # Define the matrix A as a parameter of the module\n",
    "        self.A = nn.Parameter(torch.rand(input_dim, input_dim))  # You can initialize it as you like\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the bilinear form x^T A x\n",
    "        bilinear_form = torch.matmul(x, torch.matmul(self.A, x))\n",
    "        return bilinear_form\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 3  # Change this according to your input dimension\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Create the bilinear layer\n",
    "bilinear_layer = BilinearLayer(input_dim)\n",
    "\n",
    "# Forward pass to calculate the bilinear form\n",
    "output = bilinear_layer(x)\n",
    "\n",
    "# Compute gradients\n",
    "output.backward()\n",
    "\n",
    "# Access the gradients of A\n",
    "gradient_A = bilinear_layer.A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b1732cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coupling from Spin File Different from Config\n",
      "Coupling from Spin File Different from Config\n",
      "Coupling from Spin File Different from Config\n",
      "Coupling from Spin File Different from Config\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from graph_bridges.models.generative_models.sb import SB\n",
    "from graph_bridges.configs.config_sb import SBTrainerConfig\n",
    "from graph_bridges.data.graph_dataloaders_config import EgoConfig\n",
    "from graph_bridges.data.spin_glass_dataloaders_config import ParametrizedSpinGlassHamiltonianConfig\n",
    "from graph_bridges.configs.graphs.graph_config_sb import SBConfig\n",
    "from graph_bridges.configs.config_sb import ParametrizedSamplerConfig, SteinSpinEstimatorConfig\n",
    "\n",
    "from graph_bridges.models.temporal_networks.mlp.temporal_mlp import TemporalMLPConfig\n",
    "from graph_bridges.models.backward_rates.ctdd_backward_rate_config import BackwardRateTemporalHollowTransformerConfig\n",
    "from graph_bridges.models.temporal_networks.transformers.temporal_hollow_transformers import TemporalHollowTransformerConfig\n",
    "from graph_bridges.models.trainers.sb_training import SBTrainer\n",
    "from graph_bridges.models.spin_glass.spin_utils import copy_and_flip_spins\n",
    "\n",
    "from graph_bridges.data.transforms import SpinsToBinaryTensor\n",
    "spins_to_binary = SpinsToBinaryTensor()\n",
    "\n",
    "sb_config = SBConfig(delete=True,\n",
    "                     experiment_name=\"spin_glass\",\n",
    "                     experiment_type=\"sb\",\n",
    "                     experiment_indentifier=None)\n",
    "\n",
    "#self.sb_config.data = EgoConfig(as_image=False, batch_size=2, flatten_adjacency=True,full_adjacency=True)\n",
    "num_epochs = 200\n",
    "batch_size = 2\n",
    "\n",
    "sb_config.data = ParametrizedSpinGlassHamiltonianConfig(data=\"bernoulli_small\",\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        number_of_spins=3)\n",
    "sb_config.target = ParametrizedSpinGlassHamiltonianConfig(data=\"bernoulli_small\",\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          number_of_spins=3)\n",
    "\n",
    "sb_config.temp_network = TemporalMLPConfig(time_embed_dim=12,hidden_dim=250)\n",
    "sb_config.stein = SteinSpinEstimatorConfig(stein_sample_size=200,stein_epsilon=1e-2)\n",
    "sb_config.sampler = ParametrizedSamplerConfig(num_steps=20)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "sb = SB()\n",
    "sb.create_new_from_config(sb_config,device)\n",
    "\n",
    "batchdata = next(sb.data_dataloader.train().__iter__())\n",
    "X_spins = batchdata[0]\n",
    "X_copy_spin, X_flipped_spin = copy_and_flip_spins(X_spins)\n",
    "X_spins = X_spins.to(device)\n",
    "X_copy_spin = X_copy_spin.to(device)\n",
    "X_flipped_spin = X_flipped_spin.to(device)\n",
    "\n",
    "fake_time = torch.rand((X_copy_spin.shape[0],)).to(device)\n",
    "current_model = sb.training_model\n",
    "phi_0 = current_model(X_copy_spin,fake_time)\n",
    "phi_1 = current_model(X_flipped_spin,fake_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "da389309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  1.],\n",
       "        [ 1., -1.,  1.],\n",
       "        [ 1., -1.,  1.],\n",
       "        [ 1., -1., -1.],\n",
       "        [ 1., -1., -1.],\n",
       "        [ 1., -1., -1.]], device='cuda:0')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_copy_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b408310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def compute_jacobian(model, input_data):\n",
    "    input_data.requires_grad_(True)\n",
    "    output = model(input_data)\n",
    "    batch_size, num_outputs = output.size(0), output.size(1)\n",
    "    jacobian = torch.zeros(batch_size, num_outputs, input_data.size(1))\n",
    "    \n",
    "    for i in range(num_outputs):\n",
    "        output_i = output[:, i]\n",
    "        gradient = torch.zeros_like(output_i)\n",
    "        gradient.fill_(1.0)\n",
    "        output_i.backward(gradient, retain_graph=True)\n",
    "        jacobian[:, i, :] = input_data.grad.clone()\n",
    "        input_data.grad.zero_()\n",
    "    \n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d5861c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLP\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "\n",
    "hidden_dim = 10\n",
    "batch_size = 23\n",
    "input_data = torch.randn(batch_size, input_dim)\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "X_spins = X_spins.to(torch.device(\"cpu\"))\n",
    "mlp = MLP(input_dim, hidden_dim, output_dim)\n",
    "spins = spins_to_binary(X_spins)\n",
    "copy_spin = spins_to_binary(X_copy_spin).to(cpu_device)\n",
    "flipped_spin = spins_to_binary(X_flipped_spin).to(cpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5609b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = compute_jacobian(mlp, spins)\n",
    "J = J.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "33d53a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1692,  0.0123,  0.0648],\n",
       "        [-0.1697,  0.0094,  0.0722]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a3883d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1692,  0.0123, -0.0648],\n",
       "        [ 0.1697,  0.0094,  0.0722]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(2.*spins - 1.)*J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6c9de254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0799],\n",
       "        [ 0.0142],\n",
       "        [-0.0680],\n",
       "        [ 0.1733],\n",
       "        [ 0.0166],\n",
       "        [ 0.0680]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(flipped_spin) - mlp(copy_spin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f91b39de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.],\n",
       "        [1., 0., 1.],\n",
       "        [1., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2b45b492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 0., 1.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gibbs_with_gradients(unnormalized_log_prob, current_sample, num_samples=1):\n",
    "    samples = []\n",
    "    x = current_sample.clone()  # Make a copy of the current sample\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Compute d_tilde(x)\n",
    "        d_tilde = torch.sigmoid(x)  # For binary data, Eq. 3\n",
    "        \n",
    "        # Compute q(i|x)\n",
    "        q_i_given_x = F.softmax(d_tilde, dim=0)\n",
    "        \n",
    "        # Sample i ~ q(i|x)\n",
    "        i = torch.multinomial(q_i_given_x, 1).item()\n",
    "        \n",
    "        # Flip dimension i\n",
    "        x_flipped = x.clone()\n",
    "        x_flipped[i] = 1 - x[i]\n",
    "        \n",
    "        # Compute d_tilde(x_flipped)\n",
    "        d_tilde_flipped = torch.sigmoid(x_flipped)  # For binary data, Eq. 3\n",
    "        \n",
    "        # Compute q(i|x_flipped)\n",
    "        q_i_given_x_flipped = F.softmax(d_tilde_flipped, dim=0)\n",
    "        \n",
    "        # Compute the acceptance probability\n",
    "        acceptance_prob = min(1, torch.exp(unnormalized_log_prob(x_flipped) - unnormalized_log_prob(x)) * \n",
    "                                   q_i_given_x_flipped[i] / q_i_given_x[i])\n",
    "        \n",
    "        # Accept the new sample with probability acceptance_prob\n",
    "        if torch.rand(1).item() < acceptance_prob:\n",
    "            x = x_flipped\n",
    "        \n",
    "        samples.append(x.clone())\n",
    "    \n",
    "    return torch.stack(samples)\n",
    "\n",
    "# Example usage:\n",
    "# Define your unnormalized log-prob function unnormalized_log_prob(x)\n",
    "# Initialize a current sample x_current\n",
    "# Call the gibbs_with_gradients function to perform Gibbs sampling\n",
    "# sampled_samples = gibbs_with_gradients(unnormalized_log_prob, x_current, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2dca4ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Reconstruction Error = 0.4866666793823242\n",
      "Epoch 10: Reconstruction Error = 0.4416666626930237\n",
      "Epoch 20: Reconstruction Error = 0.4933333396911621\n",
      "Epoch 30: Reconstruction Error = 0.48500001430511475\n",
      "Epoch 40: Reconstruction Error = 0.4266666769981384\n",
      "Epoch 50: Reconstruction Error = 0.4566666781902313\n",
      "Epoch 60: Reconstruction Error = 0.45500001311302185\n",
      "Epoch 70: Reconstruction Error = 0.4483333230018616\n",
      "Epoch 80: Reconstruction Error = 0.4749999940395355\n",
      "Epoch 90: Reconstruction Error = 0.43833333253860474\n",
      "Epoch 100: Reconstruction Error = 0.4483333230018616\n",
      "Epoch 110: Reconstruction Error = 0.44999998807907104\n",
      "Epoch 120: Reconstruction Error = 0.4583333432674408\n",
      "Epoch 130: Reconstruction Error = 0.46666666865348816\n",
      "Epoch 140: Reconstruction Error = 0.47833332419395447\n",
      "Epoch 150: Reconstruction Error = 0.47333332896232605\n",
      "Epoch 160: Reconstruction Error = 0.4650000035762787\n",
      "Epoch 170: Reconstruction Error = 0.46166667342185974\n",
      "Epoch 180: Reconstruction Error = 0.44999998807907104\n",
      "Epoch 190: Reconstruction Error = 0.476666659116745\n",
      "Epoch 200: Reconstruction Error = 0.46000000834465027\n",
      "Epoch 210: Reconstruction Error = 0.46666666865348816\n",
      "Epoch 220: Reconstruction Error = 0.4283333420753479\n",
      "Epoch 230: Reconstruction Error = 0.4533333480358124\n",
      "Epoch 240: Reconstruction Error = 0.4816666543483734\n",
      "Epoch 250: Reconstruction Error = 0.43666666746139526\n",
      "Epoch 260: Reconstruction Error = 0.4833333194255829\n",
      "Epoch 270: Reconstruction Error = 0.4533333480358124\n",
      "Epoch 280: Reconstruction Error = 0.44999998807907104\n",
      "Epoch 290: Reconstruction Error = 0.4816666543483734\n",
      "Epoch 300: Reconstruction Error = 0.4283333420753479\n",
      "Epoch 310: Reconstruction Error = 0.4483333230018616\n",
      "Epoch 320: Reconstruction Error = 0.4566666781902313\n",
      "Epoch 330: Reconstruction Error = 0.476666659116745\n",
      "Epoch 340: Reconstruction Error = 0.4816666543483734\n",
      "Epoch 350: Reconstruction Error = 0.4950000047683716\n",
      "Epoch 360: Reconstruction Error = 0.4516666531562805\n",
      "Epoch 370: Reconstruction Error = 0.476666659116745\n",
      "Epoch 380: Reconstruction Error = 0.45500001311302185\n",
      "Epoch 390: Reconstruction Error = 0.4833333194255829\n",
      "Epoch 400: Reconstruction Error = 0.4233333468437195\n",
      "Epoch 410: Reconstruction Error = 0.4633333384990692\n",
      "Epoch 420: Reconstruction Error = 0.44333332777023315\n",
      "Epoch 430: Reconstruction Error = 0.46666666865348816\n",
      "Epoch 440: Reconstruction Error = 0.4449999928474426\n",
      "Epoch 450: Reconstruction Error = 0.4399999976158142\n",
      "Epoch 460: Reconstruction Error = 0.4699999988079071\n",
      "Epoch 470: Reconstruction Error = 0.4699999988079071\n",
      "Epoch 480: Reconstruction Error = 0.4983333349227905\n",
      "Epoch 490: Reconstruction Error = 0.476666659116745\n",
      "Epoch 500: Reconstruction Error = 0.46833333373069763\n",
      "Epoch 510: Reconstruction Error = 0.47833332419395447\n",
      "Epoch 520: Reconstruction Error = 0.4933333396911621\n",
      "Epoch 530: Reconstruction Error = 0.4350000023841858\n",
      "Epoch 540: Reconstruction Error = 0.4833333194255829\n",
      "Epoch 550: Reconstruction Error = 0.4716666638851166\n",
      "Epoch 560: Reconstruction Error = 0.4650000035762787\n",
      "Epoch 570: Reconstruction Error = 0.4633333384990692\n",
      "Epoch 580: Reconstruction Error = 0.47833332419395447\n",
      "Epoch 590: Reconstruction Error = 0.47833332419395447\n",
      "Epoch 600: Reconstruction Error = 0.45500001311302185\n",
      "Epoch 610: Reconstruction Error = 0.4533333480358124\n",
      "Epoch 620: Reconstruction Error = 0.476666659116745\n",
      "Epoch 630: Reconstruction Error = 0.4699999988079071\n",
      "Epoch 640: Reconstruction Error = 0.47333332896232605\n",
      "Epoch 650: Reconstruction Error = 0.5\n",
      "Epoch 660: Reconstruction Error = 0.4833333194255829\n",
      "Epoch 670: Reconstruction Error = 0.4749999940395355\n",
      "Epoch 680: Reconstruction Error = 0.4699999988079071\n",
      "Epoch 690: Reconstruction Error = 0.4566666781902313\n",
      "Epoch 700: Reconstruction Error = 0.4583333432674408\n",
      "Epoch 710: Reconstruction Error = 0.4950000047683716\n",
      "Epoch 720: Reconstruction Error = 0.476666659116745\n",
      "Epoch 730: Reconstruction Error = 0.4566666781902313\n",
      "Epoch 740: Reconstruction Error = 0.4699999988079071\n",
      "Epoch 750: Reconstruction Error = 0.47833332419395447\n",
      "Epoch 760: Reconstruction Error = 0.4483333230018616\n",
      "Epoch 770: Reconstruction Error = 0.4749999940395355\n",
      "Epoch 780: Reconstruction Error = 0.46000000834465027\n",
      "Epoch 790: Reconstruction Error = 0.44999998807907104\n",
      "Epoch 800: Reconstruction Error = 0.43666666746139526\n",
      "Epoch 810: Reconstruction Error = 0.4566666781902313\n",
      "Epoch 820: Reconstruction Error = 0.46666666865348816\n",
      "Epoch 830: Reconstruction Error = 0.4650000035762787\n",
      "Epoch 840: Reconstruction Error = 0.5149999856948853\n",
      "Epoch 850: Reconstruction Error = 0.46666666865348816\n",
      "Epoch 860: Reconstruction Error = 0.4650000035762787\n",
      "Epoch 870: Reconstruction Error = 0.47833332419395447\n",
      "Epoch 880: Reconstruction Error = 0.46166667342185974\n",
      "Epoch 890: Reconstruction Error = 0.45500001311302185\n",
      "Epoch 900: Reconstruction Error = 0.4716666638851166\n",
      "Epoch 910: Reconstruction Error = 0.4583333432674408\n",
      "Epoch 920: Reconstruction Error = 0.4833333194255829\n",
      "Epoch 930: Reconstruction Error = 0.4533333480358124\n",
      "Epoch 940: Reconstruction Error = 0.476666659116745\n",
      "Epoch 950: Reconstruction Error = 0.4399999976158142\n",
      "Epoch 960: Reconstruction Error = 0.4749999940395355\n",
      "Epoch 970: Reconstruction Error = 0.4583333432674408\n",
      "Epoch 980: Reconstruction Error = 0.4483333230018616\n",
      "Epoch 990: Reconstruction Error = 0.44333332777023315\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def contrastive_divergence(rbm, input_data, k=1, learning_rate=0.1, num_epochs=100):\n",
    "    num_samples, num_visible_units = input_data.size()\n",
    "    num_hidden_units = rbm.num_hidden_units\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        positive_associations = torch.matmul(input_data.t(), rbm.sample_hidden_given_visible(input_data))\n",
    "        negative_visible_activations = rbm.sample_visible_given_hidden(rbm.sample_hidden_given_visible(input_data))\n",
    "        negative_hidden_activations = rbm.sample_hidden_given_visible(negative_visible_activations)\n",
    "        negative_associations = torch.matmul(negative_visible_activations.t(), negative_hidden_activations)\n",
    "        \n",
    "        rbm.weights += learning_rate * ((positive_associations - negative_associations) / num_samples)\n",
    "        rbm.visible_bias += learning_rate * torch.sum(input_data - negative_visible_activations, dim=0) / num_samples\n",
    "        rbm.hidden_bias += learning_rate * torch.sum(rbm.sample_hidden_given_visible(input_data) - negative_hidden_activations, dim=0) / num_samples\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            reconstruction_error = torch.mean(torch.square(input_data - negative_visible_activations))\n",
    "            print(f\"Epoch {epoch}: Reconstruction Error = {reconstruction_error.item()}\")\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, num_visible_units, num_hidden_units):\n",
    "        self.num_visible_units = num_visible_units\n",
    "        self.num_hidden_units = num_hidden_units\n",
    "        self.weights = torch.randn(num_visible_units, num_hidden_units)\n",
    "        self.visible_bias = torch.zeros(num_visible_units)\n",
    "        self.hidden_bias = torch.zeros(num_hidden_units)\n",
    "    \n",
    "    def sample_hidden_given_visible(self, visible):\n",
    "        hidden_prob = sigmoid(torch.matmul(visible, self.weights) + self.hidden_bias)\n",
    "        return torch.bernoulli(hidden_prob)\n",
    "    \n",
    "    def sample_visible_given_hidden(self, hidden):\n",
    "        visible_prob = sigmoid(torch.matmul(hidden, self.weights.t()) + self.visible_bias)\n",
    "        return torch.bernoulli(visible_prob)\n",
    "\n",
    "# Example usage:\n",
    "# Define the number of visible and hidden units\n",
    "num_visible_units = 6\n",
    "num_hidden_units = 2\n",
    "\n",
    "# Create an RBM model\n",
    "rbm = RBM(num_visible_units, num_hidden_units)\n",
    "\n",
    "# Generate some example training data\n",
    "input_data = torch.randint(2, size=(100, num_visible_units), dtype=torch.float32)\n",
    "\n",
    "# Train the RBM using Contrastive Divergence\n",
    "contrastive_divergence(rbm, input_data, k=1, learning_rate=0.1, num_epochs=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
