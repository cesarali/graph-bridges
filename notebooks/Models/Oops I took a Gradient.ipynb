{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4da79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network for f(x)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # output is a single scalar for f(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "class MatrixMultiplier(nn.Module):\n",
    "    def __init__(self, input_dim, param_value=1.0):\n",
    "        super(MatrixMultiplier, self).__init__()\n",
    "        \n",
    "        # Initialize a weight matrix with all elements set to param_value\n",
    "        self.weight = nn.Parameter(torch.full((input_dim, input_dim), param_value))\n",
    "        \n",
    "        # No bias for simplicity, can be added if necessary\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad3fd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(model, x):\n",
    "    # Ensure x is detached from any previous computation graph\n",
    "    x = x.detach().clone()\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    # Compute model's forward pass\n",
    "    y = model(x)\n",
    "    \n",
    "    # Ensure existing gradients (if any) are zeroed out\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    \n",
    "    # Compute backward pass to get gradients\n",
    "    y.backward()\n",
    "    \n",
    "    return x.grad\n",
    "\n",
    "import torch\n",
    "\n",
    "def compute_jacobian(model, x):\n",
    "    \"\"\"\n",
    "    Computes the Jacobian matrix of model with respect to x\n",
    "    \"\"\"\n",
    "    # Ensure x is detached from any previous computation graph\n",
    "    x = x.detach().clone()\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    # Initialize Jacobian as empty tensor\n",
    "    jacobian = torch.zeros(x.shape[0], x.nelement())\n",
    "    \n",
    "    # Compute model's forward pass\n",
    "    y = model(x)\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        # Ensure existing gradients (if any) are zeroed out\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "        \n",
    "        # Compute backward pass for specific output element\n",
    "        y[i].backward(retain_graph=True)\n",
    "        \n",
    "        # Each row of the Jacobian corresponds to the gradient\n",
    "        # of one output with respect to the input\n",
    "        jacobian[i] = x.grad.view(-1)\n",
    "    \n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a750bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute d_tilde(x) based on the formula\n",
    "def compute_d_tilde(model, x):\n",
    "    grad_f = compute_gradient(model, x)\n",
    "    d_tilde = -((2.*x) - 1.) * grad_f\n",
    "    return d_tilde\n",
    "\n",
    "# Flip the i-th bit of x\n",
    "def flip_bit(x, i):\n",
    "    x_flipped = x.clone()\n",
    "    x_flipped[i] = 1 - x_flipped[i]\n",
    "    return x_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac07c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main Execution\n",
    "input_dim = 10  # Example for D=10 dimensional binary data\n",
    "model = MatrixMultiplier(input_dim,param_value=20.)\n",
    "\n",
    "# Example input\n",
    "x = torch.rand(input_dim)\n",
    "x = (x > 0.5).float()  # Convert to binary\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "091c3e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200., 200., 200., 200., 200., 200., 200., 200., 200., 200.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(model,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e2d6711",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tilde = compute_d_tilde(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58f4080f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 1., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for i-th bit flipped\n",
    "i = 3  # 3rd bit as an example\n",
    "x_flipped = flip_bit(x, i)\n",
    "x_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58f07ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-200.0, -200.0, -200.0, -200.0, 200.0, -200.0, 200.0, -200.0, 200.0, 200.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_differences = []\n",
    "for i in range(input_dim):\n",
    "    x_flipped = flip_bit(x,i)\n",
    "    f_difference = model(x_flipped) - model(x)\n",
    "    all_differences.append(f_difference.item())\n",
    "all_differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df06acee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-200., -200., -200., -200.,  200., -200.,  200., -200.,  200.,  200.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tilde = compute_d_tilde(model, x)\n",
    "d_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21f7a757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_tilde: tensor([-200., -200., -200., -200.,  200., -200.,  200., -200.,  200.,  200.])\n",
      "f(x_flipped) - f(x): -200.0\n"
     ]
    }
   ],
   "source": [
    "print(\"d_tilde:\", d_tilde)\n",
    "print(\"f(x_flipped) - f(x):\", f_difference.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f701273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.\n",
    "compute_gradient(model,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741ea68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8498d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181c902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba280ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Deep EBM (from our previous discussion)\n",
    "class DeepEBM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(DeepEBM, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def compute_d_tilde(x, f):\n",
    "    gradient = torch.autograd.grad(f(x).sum(), x, create_graph=True)[0]\n",
    "    return -(2*x - 1) * gradient\n",
    "\n",
    "def gibbs_with_gradients(f, x_current):\n",
    "    # Compute d_tilde for the current sample\n",
    "    d = compute_d_tilde(x_current, f)\n",
    "\n",
    "    # Compute q based on d_tilde\n",
    "    probs = F.softmax(d / 2, dim=0)\n",
    "    i = torch.multinomial(probs, 1).item()\n",
    "\n",
    "    # Create the new sample by flipping the i-th bit\n",
    "    x_prime = x_current.clone()\n",
    "    x_prime[i] = 1 - x_current[i]\n",
    "\n",
    "    # Compute d_tilde for the new sample\n",
    "    d_prime = compute_d_tilde(x_prime, f)\n",
    "    \n",
    "    # Compute q for the new sample based on d_tilde_prime\n",
    "    probs_prime = F.softmax(d_prime / 2, dim=0)\n",
    "\n",
    "    # Accept with probability:\n",
    "    accept_prob = min(torch.exp(f(x_prime) - f(x_current)) * (probs_prime[i] / probs[i]), 1)\n",
    "    if torch.rand(1) < accept_prob:\n",
    "        return x_prime\n",
    "    else:\n",
    "        return x_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dim = 10\n",
    "ebm = DeepEBM(input_dim)\n",
    "ebm = ebm.train()\n",
    "x_sample = torch.randint(2, (input_dim,), dtype=torch.float32)  # initialize a random binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebdc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = gibbs_with_gradients(ebm, x_sample)\n",
    "\n",
    "print(\"Initial Sample:\", x_sample)\n",
    "print(\"New Sample after Gibbs with Gradients:\", new_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
